# -*- coding: utf-8 -*-
"""Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1826pfsZyVSxaKHcWRYai0NI7xLLqcvs5
"""

import pandas as pd
import joblib  # or use pickle

model = joblib.load('hist_gradient_boosting_model_depression_binary.pkl')

test_df  = pd.read_csv('308_Transcript.csv')

df=test_df

import pandas as pd
import re
import joblib



# --- Step 2: Feature extraction ---
total_duration = df['End_Time'].max() - df['Start_Time'].min()
total_utterances = len(df)
avg_confidence = df['Confidence'].mean()
total_words = df['Text'].astype(str).str.split().map(len).sum()
avg_words_per_utterance = total_words / total_utterances

start_times = df['Start_Time'].values
end_times = df['End_Time'].values
pauses = start_times[1:] - end_times[:-1]
avg_pause_duration = pauses.mean() if len(pauses) > 0 else 0
max_pause_duration = pauses.max() if len(pauses) > 0 else 0

full_text = " ".join(df['Text'].astype(str)).lower()
words = re.findall(r'\b\w+\b', full_text)
unique_words = len(set(words))
lexical_diversity = unique_words / total_words if total_words else 0
avg_word_len = sum(len(w) for w in words) / total_words if total_words else 0

sleep_keywords = ['sleep', 'tired', 'insomnia', 'rest', 'dream', 'awake', 'nap', 'fatigue']
sleep_word_count = sum(full_text.count(word) for word in sleep_keywords)

first_person_pronouns = sum(full_text.count(w) for w in ['i', 'me', 'my'])
negations = sum(full_text.count(w) for w in ['not', "don’t", "can't", "won’t", "no", "never"])

# --- Step 3: Prepare feature vector ---
features = pd.DataFrame([{
    'Total_Duration': total_duration,
    'Total_Utterances': total_utterances,
    'Avg_Confidence': avg_confidence,
    'Total_Words': total_words,
    'Avg_Words_Per_Utterance': avg_words_per_utterance,
    'Avg_Pause_Duration': avg_pause_duration,
    'Max_Pause_Duration': max_pause_duration,
    'Lexical_Diversity': lexical_diversity,
    'Avg_Word_Length': avg_word_len,
    'Sleep_Word_Count': sleep_word_count,
    'First_Person_Pronouns': first_person_pronouns,
    'Negation_Count': negations
}])

# Assuming 'features' contains the DataFrame with 'Participant_ID' and other feature columns
# Drop the 'Participant_ID' column before scaling the features
scaler = joblib.load('scaler.pkl')
#features_without_pid = features.drop(columns=['Participant_ID'])

# Apply the scaler to the features (excluding 'Participant_ID')
new_normalized_features = pd.DataFrame(scaler.transform(features_without_pid), columns=features_without_pid.columns)

# If you want to include the 'Participant_ID' column back after normalization, you can concatenate it
#new_normalized_features['Participant_ID'] = features['Participant_ID']

# Now, new_normalized_features will have normalized data, with 'Participant_ID' included as a separate column
print(new_normalized_features.head())

import pandas as pd
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer

# Load pre-trained models
with open('count_vectorizer (2).pkl', 'rb') as f:
    vectorizer = pickle.load(f)

with open('pca_model (1).pkl', 'rb') as f:
    pca = pickle.load(f)
print(f"Number of components to retain 95% variance: {pca.n_components_}")
with open('hist_gradient_boosting_model_depression_binary.pkl', 'rb') as f:  # Load your trained model (e.g., LogisticRegression, SVM)
    classifier = pickle.load(f)
# Step 1: Preprocess the text data
# Join all text entries (assuming there's a 'Text' column in your transcript)
test_text = ' '.join(test_df['Text'].dropna().astype(str).tolist())

# Step 2: Transform the test data using pre-trained CountVectorizer
X_test_bow = vectorizer.transform([test_text]).toarray()  # Transforming the single transcript

# Step 3: Standardize the test data using the pre-trained StandardScaler
X_test_scaled_df = pd.DataFrame(X_test_bow, columns=[f'word_{i}' for i in range(X_test_bow.shape[1])])
X_test_combined = pd.concat([X_test_scaled_df.reset_index(drop=True), new_normalized_features.reset_index(drop=True)], axis=1)


# Step 4: Apply PCA transformation on the scaled data
X_test_pca = pca.transform(X_test_combined)

# Step 5: Make predictions using the pre-trained model
prediction = model.predict(X_test_pca)

# Output the result
print(f"Prediction for the test transcript: {prediction[0]}")